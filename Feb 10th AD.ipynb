{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c6d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Obtaining dependency information for PyPDF2 from https://files.pythonhosted.org/packages/8e/5e/c86a5643653825d3c913719e788e41386bee415c2b87b4f955432f2de6b2/pypdf2-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/232.6 kB 326.8 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 41.0/232.6 kB 326.8 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 41.0/232.6 kB 326.8 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 122.9/232.6 kB 514.3 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 122.9/232.6 kB 514.3 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 122.9/232.6 kB 514.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 232.6/232.6 kB 592.9 kB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip  install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d89b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9483ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 35\n",
      " \n",
      " \n",
      " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
      "Acknowledgements  \n",
      "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
      "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
      "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
      "2014‐34. \n",
      " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
      " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
      " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
      " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
      " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
      " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
      " \n",
      "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
      " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
      " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
      " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
      "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
      " beginning  of the project and their \n",
      "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
      "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
      "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
      "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
      "understanding  actual ground conditions.  \n",
      " \n",
      "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
      "and anticipate  the work's usefulness  for the intended purpose. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "pdf = open(\"file1pdf.pdf\",\"rb\")\n",
    "pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "print(\"Number of pages:\",len(pdf_reader.pages))\n",
    "page = pdf_reader.pages[1]\n",
    "print(page.extract_text())\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193c8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2, urllib, nltk\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2901dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wFile = urllib.request.urlopen('http://www.udri.org/pdf/02%20working%20paper%201.pdf')\n",
    "\n",
    "pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394bd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageObj = pdfreader.pages[2]\n",
    "page2 = pageObj.extract_text()\n",
    "punctuations = ['(',')',';',':','[',']',',','...','.']\n",
    "tokens = word_tokenize(page2)\n",
    "stop_words = stopwords.words('english')\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70630cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2014‐2034',\n",
       " 'Table',\n",
       " 'Contents',\n",
       " 'The',\n",
       " 'Consultant',\n",
       " 'wishes',\n",
       " 'thank',\n",
       " 'following',\n",
       " 'individuals',\n",
       " 'Municipal',\n",
       " 'Corporation',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " 'invaluable',\n",
       " 'support',\n",
       " 'insights',\n",
       " 'contributions',\n",
       " 'towards',\n",
       " '‘',\n",
       " 'Working',\n",
       " 'Paper',\n",
       " '1',\n",
       " '–',\n",
       " 'Preparation',\n",
       " 'Base',\n",
       " 'Map',\n",
       " '’',\n",
       " 'preparation',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2014‐34',\n",
       " '.............................................................................................................................',\n",
       " '..............',\n",
       " '3',\n",
       " 'Our',\n",
       " 'gratitude',\n",
       " 'following',\n",
       " 'experts',\n",
       " 'invaluable',\n",
       " 'insights',\n",
       " 'support',\n",
       " '............................',\n",
       " '3',\n",
       " 'We',\n",
       " 'wish',\n",
       " 'especially',\n",
       " 'thank',\n",
       " 'MCGM',\n",
       " 'officers',\n",
       " 'Mr.',\n",
       " 'Jagdish',\n",
       " 'Talreja',\n",
       " 'Mr.',\n",
       " 'Dinesh',\n",
       " 'Naik',\n",
       " 'Mr.',\n",
       " 'Hiren',\n",
       " 'Daftardar',\n",
       " 'Ms.',\n",
       " 'Anita',\n",
       " 'Naik',\n",
       " 'continual',\n",
       " 'support',\n",
       " 'since',\n",
       " 'beginning',\n",
       " 'project',\n",
       " 'help',\n",
       " 'towards',\n",
       " 'familiarization',\n",
       " 'data',\n",
       " 'collection',\n",
       " 'They',\n",
       " 'instrumental',\n",
       " 'helping',\n",
       " 'contact',\n",
       " 'various',\n",
       " 'MCGM',\n",
       " 'departments',\n",
       " 'well',\n",
       " 'helping',\n",
       " 'establish',\n",
       " 'contact',\n",
       " 'personnel',\n",
       " 'government',\n",
       " 'departments',\n",
       " 'organizations',\n",
       " 'Many',\n",
       " 'thanks',\n",
       " 'MCGM',\n",
       " 'team',\n",
       " 'deploying',\n",
       " 'personnel',\n",
       " 'particularly',\n",
       " 'Mr.',\n",
       " 'Prasad',\n",
       " 'Gharat',\n",
       " 'extensive',\n",
       " 'field',\n",
       " 'visits',\n",
       " 'helped',\n",
       " 'understanding',\n",
       " 'actual',\n",
       " 'ground',\n",
       " 'conditions',\n",
       " '........................................................................................',\n",
       " '3',\n",
       " 'BEST',\n",
       " '...............................................................................................................................',\n",
       " '.................',\n",
       " '5',\n",
       " 'Brihanmumbai',\n",
       " 'Electric',\n",
       " 'Supply',\n",
       " 'Transport',\n",
       " 'Undertaking',\n",
       " '..............................................................',\n",
       " '5',\n",
       " 'CIDCO',\n",
       " '...............................................................................................................................',\n",
       " '..............',\n",
       " '5',\n",
       " 'City',\n",
       " 'Industrial',\n",
       " 'Development',\n",
       " 'Corporation',\n",
       " '...............................................................................',\n",
       " '5',\n",
       " 'CTP',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Comprehensive',\n",
       " 'Transportation',\n",
       " 'Plan',\n",
       " '...............................................................................................',\n",
       " '5',\n",
       " 'DP',\n",
       " '...............................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " '..........................................................................................................................',\n",
       " '5',\n",
       " 'DPGM34',\n",
       " '...............................................................................................................................',\n",
       " '..........',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2034',\n",
       " '.......................................................................................',\n",
       " '5',\n",
       " 'DCR',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Control',\n",
       " 'Regulations',\n",
       " '...................................................................................................',\n",
       " '5',\n",
       " 'DGPS',\n",
       " '...........................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Digital',\n",
       " 'Global',\n",
       " 'Positioning',\n",
       " 'System',\n",
       " '...................................................................................................',\n",
       " '5',\n",
       " 'DPGM',\n",
       " '...............................................................................................................................',\n",
       " '..............',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '...........................................................................................',\n",
       " '5',\n",
       " 'ELU',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Existing',\n",
       " 'Land',\n",
       " 'use',\n",
       " '.............................................................................................................................',\n",
       " '5',\n",
       " 'FSI',\n",
       " '...............................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Floor',\n",
       " 'Space',\n",
       " 'Index',\n",
       " '............................................................................................................................',\n",
       " '5',\n",
       " 'GIS',\n",
       " '...............................................................................................................................',\n",
       " '...................',\n",
       " '5']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8aacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.Jagdish Talreja', 'Mr.Dinesh Naik', 'Mr.Hiren Daftardar', 'Ms.Anita Naik', 'Mr.Prasad Gharat']\n"
     ]
    }
   ],
   "source": [
    "name_list = list()\n",
    "check =  ['Mr.', 'Mrs.', 'Ms.']\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token.startswith(tuple(check)) and idx < (len(tokens)-1):\n",
    "        name = token + tokens[idx+1] + ' ' +  tokens[idx+2]\n",
    "        name_list.append(name)\n",
    "print(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "117e45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484e4560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Obtaining dependency information for python-docx from https://files.pythonhosted.org/packages/3e/3d/330d9efbdb816d3f60bf2ad92f05e1708e4a1b9abe80461ac3444c83f749/python_docx-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\yashw\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\yashw\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/244.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/244.3 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/244.3 kB 262.6 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 61.4/244.3 kB 328.2 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 61.4/244.3 kB 328.2 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 71.7/244.3 kB 262.6 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 92.2/244.3 kB 308.0 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 133.1/244.3 kB 374.6 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 153.6/244.3 kB 416.7 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 153.6/244.3 kB 416.7 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 153.6/244.3 kB 416.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 215.0/244.3 kB 396.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 235.5/244.3 kB 244.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 244.3/244.3 kB 245.6 kB/s eta 0:00:00\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "111db155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "876378e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = open(\"File.docx\",\"rb\")\n",
    "document = docx.Document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28d012bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: \"AI-Driven Spam Detection: Leveraging NLP for Accurate Email Classification\"Batch & Section: GT-8 & GammaLiterature SurveyAbstract:Spam email detection is crucial for improving email communication and preventing security threats. This study uses Natural Language Processing (NLP) techniques to classify emails as spam or legitimate. The process starts with pre-processing steps like text cleaning, tokenization, and stop-word removal to prepare the data. Key features are extracted using methods like word frequencies, n-grams, and Term Frequency- Inverse Document Frequency (TF-IDF), along with patterns such as URLs and excessive capitalization to improve detection accuracy.Machine learning models such as Naïve Bayes, Support Vector Machines (SVM), and Logistic Regression are first used for classification. For more complex datasets, advanced models like Long Short-Term Memory (LSTM) networks and transformers like BERT are employed. The system is evaluated using metrics such as precision, recall, and F1-score to ensure reliable performance. The results show that combining effective feature extraction and modern NLP techniques significantly enhances spam detection accuracy.This solution not only automates spam filtering but also protects users from unsolicited and malicious content. Future improvements may include analyzing email metadata to furtherrefinedetectionsystemsTitle 1: \"Next-Generation Spam Filtering: Comparative Fine-Tuning of LLMs\".Authors: Nasiopoulos et al. (2024)Summary: This study addresses the persistent challenges of spam emails and phishing attacks by introducing an advanced approach to email filtering. The methodology revolves around harnessing the capabilities of advanced language models, particularly GPT-4, BERT, and RoBERTa, through fine-tuning on email datasets. The research demonstrates that fine-tuning large language models (LLMs) can significantly enhance spam detection accuracy.Key Points:The fine-tuned GPT-4 model achieved an F1-score of 95%.LLMs demonstrated superior performance in handling context and nuances in email content.The study highlights the potential of LLMs in improving email security systems.Conclusion: This research showcases how deep learning can transform sentiment analysis tasks, offering a scalable solution for businesses seeking insights into customer feedback.Title 2: \"Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection\".Authors: Maxime Labonne and Sean Moran (2023)Summary: This research investigates the effectiveness of large language models (LLMs) in email spam detection by comparing models from BERT-like, Sentence Transformers, and Seq2Seq families. The study assesses performance across public datasets in both full training and few-shot settings, introducing Spam-T5, a fine-tuned Flan-T5 model specifically adapted for spam detection.Key Points:Spam-T5 outperformed baseline models, especially in few-shot scenarios.LLMs showed adaptability to spam detection tasks with limited labeled data.The research emphasizes the suitability of LLMs for tasks requiring frequent model updates.Conclusion: LLMs, particularly the Spam-T5 model, demonstrate significant potential in spam detection, offering effective solutions in scenarios with limited training samples.Title 3: \"An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach\"Authors: Suhaima Jamal and Hayden Wimmer (2023)Summary: This study presents IPSDM, a model based on fine-tuning the BERT family of models to detect phishing and spam emails. The research demonstrates that IPSDM can better classify emails in both unbalanced and balanced datasets, highlighting the transformational potential of LLMs in enhancing email security.Key Points:IPSDM achieved higher classification accuracy compared to baseline models.The model effectively handled both unbalanced and balanced datasets.The study underscores the importance of LLMs in improving information system security.Conclusion: Fine-tuning transformer-based models like IPSDM offers a promising approach to enhancing spam and phishing email detection, contributing to improved cybersecurity measures.Title 4: \"Zero-Shot Spam Email Classification Using Pre-trained Large Language Models\".Authors: Sergio Rojas-Galeano (2024)Summary: This paper investigates the application of pre-trained large language models (LLMs) for spam email classification using zero-shot prompting. The study evaluates the performance of models like Flan-T5, ChatGPT, and GPT-4 on the SpamAssassin dataset, exploring both raw content classification and summary-based approaches.Key Points:GPT-4 achieved an F1-score of 95% using summary-based classification.LLMs demonstrated strong performance without additional training.The research highlights the potential of LLM-based pipelines for spam detection.Conclusion: This work highlights the potential for mobile applications to democratize access to AI-driven sentiment analysis.Title 5: \"SpaML: a Bimodal Ensemble Learning Spam Detector based on NLP Techniques\".Authors: Jaouhar Fattahi and Mohamed Mejri (2020)Summary: This paper introduces SpaML, a spam detection tool utilizing a combination of supervised and unsupervised classifiers, along with Natural Language Processing techniques like Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). The ensemble learning approach aims to enhance detection accuracy and precision.Key Points:SpaML demonstrated high accuracy and precision in spam detection tasks.The bimodal ensemble approach effectively combined multiple classifiers.\n"
     ]
    }
   ],
   "source": [
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "    docu += para.text\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f11ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of the paragraph 0 is ：\n",
      "\n",
      "The content of the paragraph 1 is ：\n",
      "\n",
      "The content of the paragraph 2 is ：\n",
      "\n",
      "The content of the paragraph 3 is ：Title: \"AI-Driven Spam Detection: Leveraging NLP for Accurate Email Classification\"\n",
      "\n",
      "The content of the paragraph 4 is ：Batch & Section: GT-8 & Gamma\n",
      "\n",
      "The content of the paragraph 5 is ：\n",
      "\n",
      "The content of the paragraph 6 is ：Literature Survey\n",
      "\n",
      "The content of the paragraph 7 is ：Abstract:\n",
      "\n",
      "The content of the paragraph 8 is ：Spam email detection is crucial for improving email communication and preventing security threats. This study uses Natural Language Processing (NLP) techniques to classify emails as spam or legitimate. The process starts with pre-processing steps like text cleaning, tokenization, and stop-word removal to prepare the data. Key features are extracted using methods like word frequencies, n-grams, and Term Frequency- Inverse Document Frequency (TF-IDF), along with patterns such as URLs and excessive capitalization to improve detection accuracy.\n",
      "\n",
      "The content of the paragraph 9 is ：Machine learning models such as Naïve Bayes, Support Vector Machines (SVM), and Logistic Regression are first used for classification. For more complex datasets, advanced models like Long Short-Term Memory (LSTM) networks and transformers like BERT are employed. The system is evaluated using metrics such as precision, recall, and F1-score to ensure reliable performance. The results show that combining effective feature extraction and modern NLP techniques significantly enhances spam detection accuracy.\n",
      "\n",
      "The content of the paragraph 10 is ：This solution not only automates spam filtering but also protects users from unsolicited and malicious content. Future improvements may include analyzing email metadata to furtherrefinedetectionsystems\n",
      "\n",
      "The content of the paragraph 11 is ：\n",
      "\n",
      "The content of the paragraph 12 is ：Title 1: \"Next-Generation Spam Filtering: Comparative Fine-Tuning of LLMs\".\n",
      "\n",
      "The content of the paragraph 13 is ：Authors: Nasiopoulos et al. (2024)\n",
      "\n",
      "The content of the paragraph 14 is ：Summary: This study addresses the persistent challenges of spam emails and phishing attacks by introducing an advanced approach to email filtering. The methodology revolves around harnessing the capabilities of advanced language models, particularly GPT-4, BERT, and RoBERTa, through fine-tuning on email datasets. The research demonstrates that fine-tuning large language models (LLMs) can significantly enhance spam detection accuracy.\n",
      "\n",
      "The content of the paragraph 15 is ：Key Points:\n",
      "\n",
      "The content of the paragraph 16 is ：The fine-tuned GPT-4 model achieved an F1-score of 95%.\n",
      "\n",
      "The content of the paragraph 17 is ：LLMs demonstrated superior performance in handling context and nuances in email content.\n",
      "\n",
      "The content of the paragraph 18 is ：The study highlights the potential of LLMs in improving email security systems.\n",
      "\n",
      "The content of the paragraph 19 is ：Conclusion: This research showcases how deep learning can transform sentiment analysis tasks, offering a scalable solution for businesses seeking insights into customer feedback.\n",
      "\n",
      "The content of the paragraph 20 is ：\n",
      "\n",
      "The content of the paragraph 21 is ：\n",
      "\n",
      "The content of the paragraph 22 is ：\n",
      "\n",
      "The content of the paragraph 23 is ：\n",
      "\n",
      "The content of the paragraph 24 is ：Title 2: \"Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection\".\n",
      "\n",
      "The content of the paragraph 25 is ：Authors: Maxime Labonne and Sean Moran (2023)\n",
      "\n",
      "The content of the paragraph 26 is ：Summary: This research investigates the effectiveness of large language models (LLMs) in email spam detection by comparing models from BERT-like, Sentence Transformers, and Seq2Seq families. The study assesses performance across public datasets in both full training and few-shot settings, introducing Spam-T5, a fine-tuned Flan-T5 model specifically adapted for spam detection.\n",
      "\n",
      "The content of the paragraph 27 is ：Key Points:\n",
      "\n",
      "The content of the paragraph 28 is ：Spam-T5 outperformed baseline models, especially in few-shot scenarios.\n",
      "\n",
      "The content of the paragraph 29 is ：LLMs showed adaptability to spam detection tasks with limited labeled data.\n",
      "\n",
      "The content of the paragraph 30 is ：The research emphasizes the suitability of LLMs for tasks requiring frequent model updates.\n",
      "\n",
      "The content of the paragraph 31 is ：Conclusion: LLMs, particularly the Spam-T5 model, demonstrate significant potential in spam detection, offering effective solutions in scenarios with limited training samples.\n",
      "\n",
      "The content of the paragraph 32 is ：\n",
      "\n",
      "The content of the paragraph 33 is ：Title 3: \"An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach\"\n",
      "\n",
      "The content of the paragraph 34 is ：Authors: Suhaima Jamal and Hayden Wimmer (2023)\n",
      "\n",
      "The content of the paragraph 35 is ：Summary: This study presents IPSDM, a model based on fine-tuning the BERT family of models to detect phishing and spam emails. The research demonstrates that IPSDM can better classify emails in both unbalanced and balanced datasets, highlighting the transformational potential of LLMs in enhancing email security.\n",
      "\n",
      "The content of the paragraph 36 is ：\n",
      "\n",
      "The content of the paragraph 37 is ：Key Points:\n",
      "\n",
      "The content of the paragraph 38 is ：IPSDM achieved higher classification accuracy compared to baseline models.\n",
      "\n",
      "The content of the paragraph 39 is ：The model effectively handled both unbalanced and balanced datasets.\n",
      "\n",
      "The content of the paragraph 40 is ：The study underscores the importance of LLMs in improving information system security.\n",
      "\n",
      "The content of the paragraph 41 is ：Conclusion: Fine-tuning transformer-based models like IPSDM offers a promising approach to enhancing spam and phishing email detection, contributing to improved cybersecurity measures.\n",
      "\n",
      "The content of the paragraph 42 is ：\n",
      "\n",
      "The content of the paragraph 43 is ：\n",
      "\n",
      "The content of the paragraph 44 is ：\n",
      "\n",
      "The content of the paragraph 45 is ：Title 4: \"Zero-Shot Spam Email Classification Using Pre-trained Large Language Models\".\n",
      "\n",
      "The content of the paragraph 46 is ：Authors: Sergio Rojas-Galeano (2024)\n",
      "\n",
      "The content of the paragraph 47 is ：Summary: This paper investigates the application of pre-trained large language models (LLMs) for spam email classification using zero-shot prompting. The study evaluates the performance of models like Flan-T5, ChatGPT, and GPT-4 on the SpamAssassin dataset, exploring both raw content classification and summary-based approaches.\n",
      "\n",
      "The content of the paragraph 48 is ：Key Points:\n",
      "\n",
      "The content of the paragraph 49 is ：GPT-4 achieved an F1-score of 95% using summary-based classification.\n",
      "\n",
      "The content of the paragraph 50 is ：LLMs demonstrated strong performance without additional training.\n",
      "\n",
      "The content of the paragraph 51 is ：The research highlights the potential of LLM-based pipelines for spam detection.\n",
      "\n",
      "The content of the paragraph 52 is ：Conclusion: This work highlights the potential for mobile applications to democratize access to AI-driven sentiment analysis.\n",
      "\n",
      "The content of the paragraph 53 is ：\n",
      "\n",
      "The content of the paragraph 54 is ：Title 5: \"SpaML: a Bimodal Ensemble Learning Spam Detector based on NLP Techniques\".\n",
      "\n",
      "The content of the paragraph 55 is ：Authors: Jaouhar Fattahi and Mohamed Mejri (2020)\n",
      "\n",
      "The content of the paragraph 56 is ：Summary: This paper introduces SpaML, a spam detection tool utilizing a combination of supervised and unsupervised classifiers, along with Natural Language Processing techniques like Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). The ensemble learning approach aims to enhance detection accuracy and precision.\n",
      "\n",
      "The content of the paragraph 57 is ：Key Points:\n",
      "\n",
      "The content of the paragraph 58 is ：SpaML demonstrated high accuracy and precision in spam detection tasks.\n",
      "\n",
      "The content of the paragraph 59 is ：The bimodal ensemble approach effectively combined multiple classifiers.\n",
      "\n",
      "The content of the paragraph 60 is ：\n",
      "\n",
      "The content of the paragraph 61 is ：\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(document.paragraphs)):\n",
    "    print(\"The content of the paragraph \"+ str(i)+\" is ：\" + document.paragraphs[i].text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b53ac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Obtaining dependency information for bs4 from https://files.pythonhosted.org/packages/51/bb/bf7aab772a159614954d84aa832c129624ba6c32faa559dfb200a534e50b/bs4-0.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yashw\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yashw\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dabf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28ea8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee2524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
